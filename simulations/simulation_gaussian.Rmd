---
title: "ANCOVA Comparison Simulations: Gaussian Data"
author: "Kellie Ottoboni"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE,
                      warning = FALSE,
                      tidy = TRUE, tidy.opts = list(width.cutoff = 60),
                      cache = TRUE)
#devtools::install_github("statlab/permuter")
require(permuter)
require(dplyr)
require(reshape2)
require(ggplot2)
require(xtable)
options(xtable.comment = FALSE, xtable.include.rownames = FALSE, xtable.align = "p{3.5cm}")
require(grid)
require(gridExtra)

report_theme <- theme_bw() + theme(
  axis.text.x = element_text(size = 14, angle = -45),
  axis.text.y = element_text(size = 14),
  axis.title = element_text(size = 20),
  title = element_text(size = 20),
  legend.title = element_text(size = 14),
  legend.text = element_text(size = 14),
  strip.text.x = element_text(size = 14)
)
```

# Normal data

We assume the following linear data-generating process:

$$Y_{ij1} =\beta_0Y_{ij0} + \beta_{j} + \gamma_j Z_{ij} + \varepsilon_{ij}$$

for individuals $i = 1, \dots, n_j$, $j = 1, \dots, J$.
$\beta_0$ is the coefficient for the standard normally-distributed baseline measurement $Y_{i0}$, $\beta_j$ is the mean effect of being in stratum $j$, $Z_{ij}$ is the treatment level, $\gamma_j$ is the effect of treatment in stratum $j$, and $\varepsilon_{ij}$ is an error term.
We will assume that $\beta_0 = 1$.
The observed $(Y_{ij0}, \varepsilon_{ij})$ are independent across $i$ and $j$.

Suppose there are three strata with $\beta_1 = 1, \beta_2 = 1.5,$ and $\beta_3 = 2$.
Assume that there are 16 individuals per stratum and treatment assignment is balanced, i.e. 8 people receive each treatment at each stratum.

We use two designs:

* Design 1: $\gamma_1 = \gamma_2 = \gamma_3 = 1$. This is the standard assumption of a constant, additive treatment effect.
* Design 2: $\gamma_1 = \gamma > 0$, $\gamma_2 = \gamma_3 = 0$. This is a constant, additive treatment effect in stratum 1, but no treatment effect in stratums 2 and 3. This is a simplistic case of heterogeneous treatment effects.

With these two designs, we vary the distribution of $\varepsilon$.
In the first case, we use $\varepsilon \sim N(0, 1)$ to mimic the usual ANCOVA assumptions.
In the second case, $\varepsilon \sim t(2)$ so the errors are heavy-tailed.
Thus, there are four total simulation designs.

We compare five tests:

* ANCOVA: we fit a linear model of response $Y_1$ on baseline $Y_0$, treatment $Z$, and a dummy for stratum.
* Stratified permutation: we permute treatment assignment within stratum, then take the difference in means between treated and control outcomes $Y_1$
* Differenced permutation: we do the same permutation procedure as the stratified permutation test, except we use the difference between outcome and baseline, $Y_1 - Y_0$
* Linear model (LM) permutation: we use the same stratified permutation procedure as above, except use the $t$-statistic for the coefficient on treatment in the linear regression of $Y_1$ on $Y_0$, $Z$, and stratum dummies
* Freedman-Lane test: see the other Rmd document for a full description of this procedure

Throughout our simulations, we first fix $Y_0$ and stratum ID.
Treatment $Z$ and the errors $\varepsilon$ are randomly drawn according to their respective distributions.
Then, $Y_1$ is constructed using the linear data-generating process above.
We regenerate $Z$, $\varepsilon$, and $Y_1$ 100 times for each design, then compute the empirical power of the five tests.

# Data-generation, tests, and plotting functions

```{r generate_sample}
generate_simulated_data <- function(gamma, effect, errors, n = c(16, 16, 16)){
  # Input:
  # gamma = the magnitude of the treatment effect
  # effect = "same effect" or "single stratum effect" - which strata have a tr effect > 0?
  # errors = "normal" or "heavy"
  # n = number of individuals at each stratum
  # Returns: a dataframe containing columns named Y1 (response), Y0 (baseline), Z (treatment), gamma_vec (treatment effect per individual), stratumID (stratum), stratum_effect (beta coefficient per individual), and epsilon (errors)
  
  stratumID <- rep(1:3, times = n)
  N <- sum(n)
  beta <- c(1, 1.5, 2)
  
  # What is the treatment effect?
  if(effect == "same effect"){
    gamma_vec <- rep(gamma, N)
  } else {
    gamma_vec <- rep(c(gamma, 0, 0), times = n)
  }  
  
  # Generate errors
  if(errors == "normal"){
    epsilon <- rnorm(N)
  } else {
    epsilon <- rt(N, df = 2)
  }
  
  # Generate covariates
  Y0 <- rnorm(N)
  Z <- rep(0:1, length.out = N)
  stratum_effect <- rep(beta, times = n)
  Y1 <- Y0 + gamma_vec*Z + stratum_effect + epsilon
  return(data.frame(Y1, Y0, Z, gamma_vec, stratumID, stratum_effect, epsilon))
}

generate_simulated_pvalues <- function(dataset, reps = 1e3){
  # Inputs:
  # dataset = a dataframe containing columns named Y1 (response), Y0 (baseline), Z (treatment), and stratumID (stratum)
  # Returns: a vector of p-values
  # first element is the p-value from the ANCOVA
  # second element is the p-value from the stratified two-sample permutation test
  # third element is the p-value from the linear model test, permuting treatment
  # fourth element is the p-value from the Freedman-Lane linear model test, permuting residuals
  
  # ANCOVA
  modelfit <- lm(Y1 ~ Y0 + Z + factor(stratumID), data = dataset)
  resanova <- summary(aov(modelfit))
  anova_pvalue <- resanova[[1]]["Z", "Pr(>F)"]
  
  # Stratified permutation test of Y1
  observed_diff_means <- mean(dataset$Y1[dataset$Z == 1]) - mean(dataset$Y1[dataset$Z == 0])
  diff_means_distr <- stratified_two_sample(group = dataset$Z, response = dataset$Y1, stratum = dataset$stratumID, reps = reps)
  perm_pvalue <- t2p(observed_diff_means, diff_means_distr, alternative = "two-sided")
  
  # Diffed permutation test of Y1-Y0
  dataset$diff <- dataset$Y1 - dataset$Y0
  observed_diff_means2 <- mean(dataset$diff[dataset$Z == 1]) - mean(dataset$diff[dataset$Z == 0])
  diff_means_distr2 <- stratified_two_sample(group = dataset$Z, response = dataset$diff, stratum = dataset$stratumID, reps = reps)
  perm_pvalue2 <- t2p(observed_diff_means2, diff_means_distr2, alternative = "two-sided")
    
  # Permutation of treatment in linear model
  observed_t1 <- summary(modelfit)[["coefficients"]]["Z", "t value"]
  lm1_t_distr <-  replicate(reps, {
    dataset$Z_perm <- permute_within_groups(dataset$Z, dataset$stratumID)
    lm1_perm <- lm(Y1 ~ Y0 + Z_perm + factor(stratumID), data = dataset)
    summary(lm1_perm)[["coefficients"]]["Z_perm", "t value"]
  })
  lm_pvalue <- t2p(observed_t1, lm1_t_distr, alternative = "two-sided")

  # Freedman-Lane linear model residual permutation
  lm2_no_tr <- lm(Y1~Y0 + factor(stratumID), data = dataset)
  lm2_resid <- residuals(lm2_no_tr)
  lm2_yhat <- fitted(lm2_no_tr)
  lm2_t_distr <-  replicate(reps, {
    lm2_resid_perm <- permute_within_groups(lm2_resid, dataset$stratumID)
    dataset$response_fl <-  lm2_yhat + lm2_resid_perm
    lm2_perm <- lm(response_fl ~ Y0 + Z + factor(stratumID), data = dataset)
    summary(lm2_perm)[["coefficients"]]["Z", "t value"]
  })
  fl_pvalue <- t2p(observed_t1, lm2_t_distr, alternative = "two-sided")
  
  return(c("ANCOVA" = anova_pvalue, 
           "Stratified Permutation" = perm_pvalue, 
           "Differenced Permutation" = perm_pvalue2,
           "LM Permutation" = lm_pvalue, 
           "Freedman-Lane" = fl_pvalue))
}
```

```{r function_power_curves}
compute_power <- function(pvalues){
  sapply((0:99)/100, function(p) mean(pvalues <= p, na.rm = TRUE))
}

plot_power_curves <- function(power_mat, title){
  melt(power_mat) %>% 
  mutate("pvalue" = Var1/100) %>%
  mutate("Method" = Var2) %>%
  ggplot(aes_string(x = "pvalue", y = "value", color = "Method")) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  xlab("P-value") +
  ylab("Power") +
  ggtitle(title) +
  theme_bw() +
  theme(
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    axis.title = element_text(size = 16),
    title = element_text(size = 16),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 14),
    strip.text.x = element_text(size = 12)
  ) 
}
```

## Test level: simulation under the null

Before testing for different kinds of effects, we begin checking that the tests have the correct level.
We follow the procedure described above, using an effect size of $\gamma = 0$ at all strata and using standard normal errors.
To have the correct level means that the test rate of rejection at level $\alpha$ is $100\alpha\%$.
In other words, the p-values are uniformly distributed and the power curve should coincide with the line with slope $1$ through the origin.
The figure below demonstrates that this is the case.
If anything, the differenced stratified permutation test has fewer than $100\alpha\%$ false positives when using level $\alpha$.

```{r design_null_gaussian, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 0, effect = "same effect", errors = "normal")
design0_pvalues <- replicate(1000, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design0_pvalues <- t(design0_pvalues)
colnames(design0_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design0_power <- apply(design0_pvalues, 2, compute_power)
```

```{r design_null_plots, fig.height = 4, fig.align="center"}
plot_power_curves(design0_power, "No Effect, Normal Errors")
```

```{r design_null_heavy, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 0, effect = "same effect", errors = "heavy")
design00_pvalues <- replicate(1000, {
  tmp$epsilon <- rt(nrow(tmp), df = 2)
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design00_pvalues <- t(design00_pvalues)
colnames(design00_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design00_power <- apply(design00_pvalues, 2, compute_power)
```

```{r design_null_heavy_plots, fig.height = 4, fig.align="center"}
plot_power_curves(design00_power, "No Effect, Heavy-tailed Errors")
```

## Design 1: Constant additive effect, normal errors

There is no discernable difference in power between the ANCOVA, the differenced stratified permutation test, the LM permutations, or the Freedman-Lane test.
However, the simple stratified permutation test of $Y_1$ has substantially less power than the other four.
Without controlling for the baseline values, the variance in $Y_1$ masks the treatment effect.

```{r design1, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "same effect", errors = "normal")
design1_pvalues <- replicate(1000, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})

design1_pvalues <- t(design1_pvalues)
colnames(design1_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design1_power <- apply(design1_pvalues, 2, compute_power)
```

```{r design1_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design1_power, "Constant Additive Effect of 1 \n Normal Errors")
```

## Design 2: Single stratum effect, normal errors

A similar pattern emerges: the simple stratified permutation test has low power, while the other four power curves roughly coincide.
The Freedman-Lane test may have the highest power for small p-values, but this could also just be noise.
All five power curves are closer to the line passing through the origin: since the effect is only present at one stratum, it is more difficult to detect.

```{r design2, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "single stratum effect", errors = "normal")
design2_pvalues <- replicate(1000, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design2_pvalues <- t(design2_pvalues)
colnames(design2_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design2_power <- apply(design2_pvalues, 2, compute_power)
```

```{r design2_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design2_power, "Effect of 1 at a Single stratum \n Normal Errors")
```

## Design 3: Constant additive effect, heavy-tailed errors

Again, we find that the four power curves coincide while the curve for the stratified permutation test is below the others.
Here, the difference between the curves is not large.
Controlling for baseline $Y_0$ does not substantially help reduce variance.

```{r design3, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "same effect", errors = "heavy")
design3_pvalues <- replicate(1000, {
  tmp$epsilon <- rt(nrow(tmp), df = 2)
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design3_pvalues <- t(design3_pvalues)
colnames(design3_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design3_power <- apply(design3_pvalues, 2, compute_power)
```

```{r design3_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design3_power, "Constant Additive Effect of 1 \n Heavy-tailed Errors")
```

## Design 4: single stratum effect, heavy-tailed errors

The noise from the heavy-tailed errors masks the treatment effect so much that controlling for baseline measures makes no difference.
There is almost no power to detect an effect using any of the five tests.

```{r design4, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "single stratum effect", errors = "heavy")
design4_pvalues <- replicate(1000, {
  tmp$epsilon <- rt(nrow(tmp), df = 2)
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design4_pvalues <- t(design4_pvalues)
colnames(design4_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design4_power <- apply(design4_pvalues, 2, compute_power)
```

```{r design4_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design4_power, "Effect of 1 at a Single stratum \n Heavy-tailed Errors") 
```


## Design 5: Unpredictive baseline

Suppose that the baseline measure is not strongly predictive of the outcome.
Then, we'd expect that controlling for baseline will not improve the power of the test, and may even make it worse.
Suppose that in the linear data-generating process above, we have $\beta_0 = 0.25$, meaning that $Y_1$ and $Y_0$ have a correlation of $0.25$.

The power curves for this design look similar to those from Design 1 with one important difference: 
the stratified permutation test has power very close to ANCOVA and the two linear model tests,
while the differenced permutation test has lower power.
This suggests that one should be careful when incorporating control variables;
naively taking the difference $Y_1 - Y_0$ does not capture the correct relationship between baseline and outcome.


```{r design5, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "same effect", errors = "normal")
design5_pvalues <- replicate(100, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- 0.25*tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})

design5_pvalues <- t(design5_pvalues)
colnames(design5_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design5_power <- apply(design5_pvalues, 2, compute_power)
```

```{r design5_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design5_power, "Constant Additive Effect of 1 \n Normal Errors \n Unpredictive baseline")
```


# Imbalanced designs

Suppose that instead of having 16 individuals per stratum, we distribute them unequally across strata:
stratum 1 has 8 patients, stratum 2 has 16 patients, and stratum 3 has 24 patients.
This imbalance does not violate any assumptions of the ANCOVA.
However, it may reduce power if the effect is concentrated in strata with fewer patients.

## Design 6: Constant additive effect, normal errors

The power curves here are very similar to those in Design 1.
This result is expected, since the effect is still present among all patients who received treatment.

```{r design6, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "same effect", errors = "normal", n = c(8, 16, 24))
design6_pvalues <- replicate(100, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})

design6_pvalues <- t(design6_pvalues)
colnames(design6_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design6_power <- apply(design6_pvalues, 2, compute_power)
```

```{r design6_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design6_power, "Constant Additive Effect of 1 \n Normal Errors")
```

## Design 7: Single stratum effect, normal errors

There is very low power to detect the effect here, since in the simulated data, the stratum with only 8 patients was the only stratum with a nonzero effect of treatment.
The treatment effect is on the same order of magnitude as the error variance.

```{r design7, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC

tmp <- generate_simulated_data(gamma = 1, effect = "single stratum effect", errors = "normal", n = c(8, 16, 24))
design7_pvalues <- replicate(100, {
  tmp$epsilon <- rnorm(nrow(tmp))
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design7_pvalues <- t(design7_pvalues)
colnames(design7_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design7_power <- apply(design7_pvalues, 2, compute_power)
```

```{r design7_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design7_power, "Effect of 1 at a Single stratum \n Normal Errors")
```

# Heteroskedastic errors

One assumption of ANCOVA is that errors are homoskedastic: they have the same variance across groups.
Suppose that this does not hold in our data.
Imagine we have three strata with 16 patients each, but they have different error distributions:
stratum 1 has standard normal errors,
stratum 2 has normally distributed errors with mean 0 and variance 2,
and stratum 3 has errors with a $t$-distribution on 2 degrees of freedom.

## Design 8: heteroskedastic errors, constant treatment effect

```{r design8, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC
nn <- 16

tmp <- generate_simulated_data(gamma = 1, effect = "same effect", errors = "normal", n = rep(nn, 3))
design8_pvalues <- replicate(100, {
  tmp$epsilon[tmp$stratumID == 1] <- rnorm(nn)
  tmp$epsilon[tmp$stratumID == 2] <- rnorm(nn, sd = sqrt(2))
  tmp$epsilon[tmp$stratumID == 3] <- rt(nn, df = 2)
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})

design8_pvalues <- t(design8_pvalues)
colnames(design8_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design8_power <- apply(design8_pvalues, 2, compute_power)
```

```{r design8_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design8_power, "Constant Additive Effect of 1 \n Heteroskedastic Errors")
```

## Design 9: heteroskedastic errors, single stratum effect


```{r design9, fig.height = 4}
set.seed(760682460) # Generated from random.org Timestamp: 2016-11-14 10:21:12 UTC
nn <- 16

tmp <- generate_simulated_data(gamma = 1, effect = "single stratum effect", errors = "normal", n = rep(nn, 3))
design9_pvalues <- replicate(100, {
  tmp$epsilon[tmp$stratumID == 1] <- rnorm(nn)
  tmp$epsilon[tmp$stratumID == 2] <- rnorm(nn, sd = sqrt(2))
  tmp$epsilon[tmp$stratumID == 3] <- rt(nn, df = 2)
  tmp$Z <- permute_within_groups(tmp$Z, tmp$stratumID)
  tmp$Y1 <- tmp$Y0 + tmp$stratum_effect + tmp$gamma_vec*tmp$Z + tmp$epsilon
  generate_simulated_pvalues(tmp)
})
design9_pvalues <- t(design9_pvalues)
colnames(design9_pvalues) <- c("ANCOVA", "Stratified Permutation", "Differenced Permutation", "LM Permutation", "Freedman-Lane")
design9_power <- apply(design9_pvalues, 2, compute_power)
```

```{r design9_plot, fig.height = 4, fig.align="center"}
plot_power_curves(design9_power, "Effect of 1 at a Single stratum \n Heteroskedastic Errors")
```



```{r final_summary, fig.align = "center", fig.width = 12}
powers <- list(design1_power %>% as.data.frame() %>% mutate("Treatment" = rep("Constant Additive Effect", nrow(design1_power)), "Errors" = rep("Gaussian", nrow(design1_power))), 
               design2_power %>% as.data.frame() %>% mutate("Treatment" = rep("Single Stratum Effect", nrow(design2_power)), "Errors" = rep("Gaussian", nrow(design2_power))), 
               design3_power %>% as.data.frame() %>% mutate("Treatment" = rep("Constant Additive Effect", nrow(design3_power)), "Errors" = rep("Heavy-tailed", nrow(design3_power))), 
               design4_power %>% as.data.frame() %>% mutate("Treatment" = rep("Single Stratum Effect", nrow(design4_power)), "Errors" = rep("Heavy-tailed", nrow(design4_power))),
               design0_power %>% as.data.frame() %>% mutate("Treatment" = rep("No Effect", nrow(design0_power)), "Errors" = rep("Gaussian", nrow(design0_power))),
              design00_power %>% as.data.frame() %>% mutate("Treatment" = rep("No Effect", nrow(design00_power)), "Errors" = rep("Heavy-tailed", nrow(design00_power))))


all_power_curves <- do.call(rbind, powers)
twobytwo <- all_power_curves %>%
  filter(Treatment != "No Effect") %>%
  melt(id.vars = c("Treatment", "Errors")) %>% 
  mutate("pvalue" = rep((1: 100)/100, 5*4)) %>%
  mutate("Method" = variable) %>%
  ggplot(aes_string(x = "pvalue", y = "value", color = "Method")) +
  geom_line() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  xlab("Significance Level") +
  ylab("Power") +
  facet_wrap(Treatment~Errors) +
  ggtitle("Gaussian Data") +
  report_theme +
  theme(legend.position = "bottom") + 
  guides(color=guide_legend(nrow=2,byrow=TRUE))
twobytwo

pdf(file = "../ms/fig/normal_simulation_power.pdf", width = 8)
twobytwo 
dev.off()
```


```{r power_summary_tab, results = "asis"}
summary05 <- t(sapply(powers, function(x) x[5, c(7, 6, 1:5)]))
summary05 <- summary05[c(5, 1, 2, 6, 3, 4), ]
summarytab <- xtable(summary05, digits = 3, align = "rp{1.15in}|p{0.7in}|p{0.6in}p{0.8in}p{0.8in}p{0.8in}p{0.75in}",
       caption = "Empirical power at level $0.05$ for Gaussian simulated data",
       label = "tab:normal_power") 
print(summarytab, hline.after = c(-1, 0, 3, nrow(summarytab)), type="latex", file="../ms/fig/normal_simulation_power_summary.tex")
summarytab
```